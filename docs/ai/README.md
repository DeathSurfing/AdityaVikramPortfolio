# ğŸ¤– AI Assistant Documentation

> Complete guide for setting up and managing your AI-powered chat assistant

## ğŸ¯ Overview

Your portfolio features an intelligent chat assistant powered by Ollama that can answer questions about your professional background, projects, and experience.

```
AI Stack
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚  FastAPI Proxy  â”‚    â”‚     Ollama      â”‚
â”‚   Chat UI       â”‚â—„â”€â”€â–ºâ”‚  CORS + Routing â”‚â—„â”€â”€â–ºâ”‚   gpt-oss:20b   â”‚
â”‚   (React)       â”‚    â”‚   (Python)      â”‚    â”‚  (Local Model)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Available Guides

### ğŸš€ Setup Guides
- [**OLLAMA_SETUP.md**](OLLAMA_SETUP.md) - Complete setup with Cloudflare Tunnel
- [**PROXY_SETUP.md**](PROXY_SETUP.md) - FastAPI proxy configuration
- [**AI_QUICK_START.md**](AI_QUICK_START.md) - Get AI assistant running in 5 minutes

### ğŸ”§ Configuration Guides
- [**CUSTOMIZATION.md**](CUSTOMIZATION.md) - Customize AI personality and responses
- [**MODELS.md**](MODELS.md) - Working with different Ollama models
- [**SYSTEM_PROMPTS.md**](SYSTEM_PROMPTS.md) - Crafting effective system prompts

### ğŸ§ª Testing & Debugging
- [**TESTING.md**](TESTING.md) - AI assistant testing strategies
- [**DEBUGGING.md**](DEBUGGING.md) - Common AI issues and solutions

## âš¡ Quick Start

### 1. Install Ollama
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama
ollama serve

# Pull the model
ollama pull gpt-oss:20b
```

### 2. Deploy AI Proxy
```bash
# Navigate to proxy directory
cd ollama-proxy

# Quick deployment
./deploy.sh

# Verify
curl http://localhost:5950/health
```

### 3. Test AI Chat
```bash
# Test chat functionality
curl -X POST http://localhost:5950/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Tell me about Aditya"}'
```

## ğŸ¨ Key Features

### AI Assistant Capabilities
- âœ… **Knowledge Base**: Trained on your professional background
- âœ… **Real-time Chat**: Instant responses via streaming API
- âœ… **CORS Support**: Works seamlessly with web browsers
- âœ… **Professional Tone**: Maintains appropriate communication style
- âœ… **Project Aware**: Knows about your specific projects and skills

### Technical Features
- âœ… **FastAPI Proxy**: High-performance async API
- âœ… **Docker Support**: Containerized deployment
- âœ… **Health Monitoring**: Built-in health checks
- âœ… **Error Handling**: Graceful fallback strategies
- âœ… **Streaming Support**: Real-time response streaming

## ğŸ“ File Structure

```
ai/
â”œâ”€â”€ README.md                 # This overview
â”œâ”€â”€ OLLAMA_SETUP.md          # Complete Ollama setup guide
â”œâ”€â”€ PROXY_SETUP.md           # FastAPI proxy documentation
â”œâ”€â”€ CUSTOMIZATION.md         # AI personality customization
â”œâ”€â”€ MODELS.md                # Model management
â”œâ”€â”€ SYSTEM_PROMPTS.md        # Prompt engineering
â”œâ”€â”€ TESTING.md               # Testing strategies
â””â”€â”€ DEBUGGING.md             # Troubleshooting guide

ollama-proxy/
â”œâ”€â”€ main.py                  # FastAPI server with system prompt
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ docker-compose.yml      # Docker Compose setup
â”œâ”€â”€ deploy.sh               # Quick deployment script
â””â”€â”€ README.md               # Proxy-specific documentation
```

## ğŸ”§ Common Tasks

### Update AI Knowledge
```python
# Edit ollama-proxy/main.py
SYSTEM_PROMPT = """
You are Aditya Vikram Mahendru's AI assistant...
[Add new experiences, projects, skills here]
"""
```

### Change AI Model
```bash
# Pull new model
ollama pull llama3.2

# Update docker-compose.yml
environment:
  - OLLAMA_MODEL=llama3.2
```

### Test AI Responses
```bash
# Health check
curl http://localhost:5950/health

# Chat test
curl -X POST http://localhost:5950/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What are your main skills?"}'

# Streaming test
curl -X POST http://localhost:5950/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"message": "Tell me about your projects"}'
```

## ğŸ¯ AI Assistant Personality

Your AI assistant is configured with:

### Professional Focus
- **Role**: Technical professional and project showcase
- **Tone**: Friendly but professional
- **Knowledge**: Your specific experience, skills, and projects
- **Style**: Conversational yet informative

### Current Knowledge Base
- ğŸ“ **Education**: Woxsen University AI-RC internship
- ğŸ’» **Skills**: Full-stack development, AI/ML, cloud platforms
- ğŸš€ **Projects**: Student Council system, MUN platform, Laundry Management
- ğŸ› ï¸ **Technologies**: Next.js, Python, Rust, Docker, AWS

## ğŸ“Š Performance & Monitoring

### Response Times
- **Health Check**: < 200ms
- **Simple Queries**: < 2 seconds
- **Complex Responses**: < 5 seconds

### Resource Usage
- **CPU**: Moderate during inference
- **Memory**: ~2GB for gpt-oss:20b model
- **Storage**: ~11GB for model files

### Health Monitoring
```bash
# Monitor AI assistant health
./scripts/manage-portfolio.sh health

# Check Ollama status
ollama ps

# View proxy logs
docker-compose logs ollama-proxy
```

## ğŸ”„ Updates & Maintenance

### Regular Tasks
- **Weekly**: Review AI responses for accuracy
- **Monthly**: Update system prompt with new experiences
- **Quarterly**: Consider model upgrades
- **As needed**: Fine-tune responses based on user feedback

### Model Updates
```bash
# Check for model updates
ollama list

# Update model
ollama pull gpt-oss:20b

# Restart proxy
./scripts/manage-portfolio.sh restart
```

## ğŸš¨ Troubleshooting

### Common Issues
1. **AI not responding**: Check Ollama service status
2. **CORS errors**: Verify proxy CORS configuration  
3. **Slow responses**: Monitor system resources
4. **Inconsistent answers**: Update system prompt

### Quick Fixes
```bash
# Restart all AI services
sudo systemctl restart ollama
./scripts/manage-portfolio.sh restart

# Check logs for errors
journalctl -u ollama -f
docker-compose logs ollama-proxy
```

---

**ğŸ¤– Your AI assistant is ready to showcase your professional expertise!** Use these guides to maintain and improve your AI-powered portfolio experience.
