# âš¡ Quick Start: Ollama + FastAPI + Cloudflare Tunnel

## ğŸ¯ What We Built

A complete CORS-enabled solution for your Ollama-powered portfolio chatbot:

```
Browser â†’ Cloudflare Tunnel â†’ FastAPI Proxy â†’ Ollama â†’ AI Response
```

## ğŸš€ Start Everything (Quick Commands)

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start FastAPI Proxy
cd ollama-proxy
./start.fish

# Terminal 3: Start Frontend (if not running)
cd frontend
npm run dev

# Terminal 4: Start Cloudflare Tunnel
cloudflared tunnel --config cloudflare-tunnel-config.yml run
```

## ğŸ§ª Test Everything

```bash
# Run the comprehensive test
./test_setup.sh
```

## ğŸŒ Your Cloudflare Tunnel Config

```yaml
ingress:
  # AI Chat API (FastAPI Proxy)
  - hostname: portfolio.adityavikram.dev
    path: /api/*
    service: http://localhost:5950

  # Portfolio Website
  - hostname: portfolio.adityavikram.dev
    service: http://localhost:8594
    
  # ... other routes
```

## ğŸ“ File Structure

```
AdityaVikramPortfolio/
â”œâ”€â”€ ollama-proxy/                # ğŸ†• FastAPI CORS proxy
â”‚   â”œâ”€â”€ main.py                  # Main FastAPI server
â”‚   â”œâ”€â”€ start.fish              # Fish shell startup
â”‚   â””â”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ frontend/                   # Your existing Next.js app
â”œâ”€â”€ cloudflare-tunnel-config.yml # Tunnel configuration
â”œâ”€â”€ test_setup.sh              # Test script
â””â”€â”€ OLLAMA_SETUP.md            # Detailed documentation
```

## âœ… What This Solves

1. **CORS Issues**: Ollama doesn't support CORS â†’ FastAPI proxy adds CORS headers
2. **Web Accessibility**: Local Ollama â†’ Exposed via Cloudflare Tunnel
3. **Same Experience**: Your existing frontend code works unchanged
4. **Performance**: Async FastAPI with connection pooling

## ğŸ‰ Result

Visit https://portfolio.adityavikram.dev, click "Try AI Assistant", and your Ollama-powered chatbot works perfectly from the web!

---

**Need help?** Check `OLLAMA_SETUP.md` for detailed documentation.
